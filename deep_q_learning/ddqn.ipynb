{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd072f82c1a1813237a015b4f4b2159de22a2fabef90219bd259332cc8881f19581",
   "display_name": "Python 3.9.2 64-bit ('py3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "env_id = \"BreakoutNoFrameskip-v4\"\n",
    "algorithm = \"ddqn\"\n",
    "max_episode = 7000\n",
    "\n",
    "seed = 42\n",
    "minibatch_size = 32\n",
    "replay_memory_size = 100000\n",
    "replay_start_size = 50000\n",
    "action_repeat = 4\n",
    "agent_history_length = 4\n",
    "update_frequency = 4\n",
    "target_network_update_frequency = 10000\n",
    "learning_rate = 0.00001\n",
    "discount_factor = 0.99\n",
    "initial_exploration = 1.0\n",
    "final_exploration = 0.1\n",
    "final_exploration_frame = 100000.0\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "weights_dir_tmpl = \"{}/{}-{}/{}\".format(\"checkpoints\", algorithm, now, {})\n",
    "log_dir = \"{}/{}-{}\".format(\"runs\", algorithm, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.pardir)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from utils.agent import Agent\n",
    "from utils.models import create_dqn_model\n",
    "from utils.policies import epsilon_greedy_policy\n",
    "from utils.algorithms import train_ddqn\n",
    "from utils.experience_replay import ExperienceReplay\n",
    "from atari.wrapper import wrapper\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "env = wrapper(env_id, skip=action_repeat, stack=agent_history_length, seed=seed)\n",
    "buffer = ExperienceReplay(memory_size=replay_memory_size, batch_size=minibatch_size)\n",
    "agent = Agent(\n",
    "    model=create_dqn_model(env.action_space.n),\n",
    "    num_actions=env.action_space.n,\n",
    "    behavior_policy=epsilon_greedy_policy,\n",
    "    train=train_ddqn,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    epsilon_init=initial_exploration,\n",
    "    epsilon_fin=final_exploration,\n",
    "    epsilon_fin_frame=final_exploration_frame)\n",
    "\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "writer.set_as_default()\n",
    "\n",
    "is_gpu = tf.config.list_physical_devices('GPU')\n",
    "metric_q = tf.keras.metrics.Mean(\"Avg. Q-value\", dtype=tf.float32)\n",
    "metric_loss = tf.keras.metrics.Mean(\"Avg. Loss\", dtype=tf.float32)\n",
    "metric_rewards = tf.keras.metrics.Sum(\"Sum. Rewards\", dtype=tf.float32)"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reward = -float('inf')\n",
    "\n",
    "running = True\n",
    "step_count = 0\n",
    "episode_count = 0\n",
    "\n",
    "while running:    \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while done is False:\n",
    "        step_count += 1\n",
    "\n",
    "        if buffer.get_history_length() < replay_start_size:\n",
    "            action = agent.get_action(state, 1.0)\n",
    "        else:\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        buffer.put(state, [action], state_next, [reward], [done])\n",
    "\n",
    "        if step_count >= replay_start_size:\n",
    "            for _ in range(update_frequency):\n",
    "                state_sample, action_sample, state_next_sample, rewards_sample, done_sample = buffer.get()\n",
    "                loss, q_max = agent.train(state_sample, action_sample, state_next_sample, rewards_sample, done_sample)\n",
    "                metric_loss(loss)\n",
    "                metric_q(q_max)\n",
    "        \n",
    "            if step_count % target_network_update_frequency == 0:\n",
    "                agent.update_model_target()\n",
    "\n",
    "        metric_rewards(reward)\n",
    "        state = state_next\n",
    "\n",
    "    if env.was_real_done:\n",
    "        episode_count += 1\n",
    "        \n",
    "        if max_reward <= metric_rewards.result():\n",
    "            max_reward = metric_rewards.result()\n",
    "            agent.save_model(weights_dir_tmpl.format(max_reward))\n",
    "        \n",
    "        if max_episode <= episode_count:\n",
    "            agent.save_model(weights_dir_tmpl.format(\"latest\"))\n",
    "            running = False\n",
    "\n",
    "        tf.summary.scalar(\"Performance/Q-value\", metric_q.result(), step=episode_count)\n",
    "        tf.summary.scalar(\"Performance/Loss\", metric_loss.result(), step=episode_count)\n",
    "        tf.summary.scalar(\"Performance/Rewards\", metric_rewards.result(), step=episode_count)\n",
    "        tf.summary.scalar(\"Etc./Epsilon\", agent.epsilon, step=episode_count)\n",
    "        if is_gpu:\n",
    "            tf.summary.scalar(\n",
    "                \"Etc./GPU usages\",\n",
    "                tf.config.experimental.get_memory_usage(\"GPU:0\"),\n",
    "                step=episode_count)\n",
    "\n",
    "        print(\"Episode: {} | Loss: {:.4f} | Q-value: {:.4f} | Rewards: {:4f} | epsilon: {:.4f} | memory size: {:}\".format(\n",
    "            episode_count, metric_loss.result(), metric_q.result(), metric_rewards.result(), agent.epsilon, buffer.get_history_length()))\n",
    "\n",
    "        metric_q.reset_states()\n",
    "        metric_loss.reset_states()\n",
    "        metric_rewards.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}