{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd072f82c1a1813237a015b4f4b2159de22a2fabef90219bd259332cc8881f19581",
   "display_name": "Python 3.9.2 64-bit ('py3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DQN\n",
    "\n",
    "## 참고 논문\n",
    "- Playing Atari with Deep Reinforcement Learning\n",
    "- Human-level control through deep reinforcement learning\n",
    "- [Frame Skipping and Pre-Processing for Deep Q-Networks on Atari 2600 Games](https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/)\n",
    "\n",
    "## Action\n",
    "- 0: None\n",
    "- 1: Fire\n",
    "- 2: Right\n",
    "- 3: Left"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preinstallation\n",
    "\n",
    "OpenAI Gym에서 \"Breakout-v4\" 게임을 플레이하기 위해서는 `atari-py`를 설치해야 한다.\n",
    "\n",
    "```Python\n",
    "pip install gym atari-py\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 게임 환경에 대한 이미지 전처리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Breakout-v4\")\n",
    "\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "action = 1\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, figsize=(16, 16*3))\n",
    "\n",
    "step = 0\n",
    "for col in range(4):\n",
    "    for _ in range(4):\n",
    "        step += 1\n",
    "        obs, _, _, info = env.step(action)\n",
    "\n",
    "    ax = axs[col]\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"step: {}, lives: {}\".format(step, info['ale.lives']))\n",
    "    ax.imshow(obs)\n",
    "\n",
    "print(obs.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def preprocessing(image):\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    # cropping an 84 × 84 region of the image that roughly captures the playing area\n",
    "    (left, upper, right, lower) = (0, 17, 160, 17+177)\n",
    "    image = image.crop(box=(left, upper, right, lower))\n",
    "\n",
    "    # converting their RGB representation to gray-scale\n",
    "    image = image.convert(\"L\")\n",
    "    \n",
    "    # down-sampling it to a 110×84 image\n",
    "    (width, height) = (84, 84)\n",
    "    image = image.resize(size=(width, height))\n",
    "\n",
    "    # normalization\n",
    "    image = np.asarray(image, dtype=np.float32)\n",
    "    image /= 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v4\")\n",
    "\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, figsize=(16, 16))\n",
    "\n",
    "step = 0\n",
    "for col in range(4):\n",
    "    for _ in range(4):\n",
    "        step += 1\n",
    "        obs, _, _, _ = env.step(action)\n",
    "\n",
    "    # 전처리를 추가\n",
    "    preprocessed = preprocessing(obs)\n",
    "\n",
    "    ax = axs[col]\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"step: {}\".format(step))\n",
    "    ax.imshow(preprocessed)\n",
    "\n",
    "print(obs.shape)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.activations import relu\n",
    "\n",
    "\n",
    "def create_model(num_actions, trainable=True):\n",
    "    # The input to the neural network consists of an 84 x 84 x 4 image produced by the preprocessing map φ.\n",
    "    inputs = Input(shape=(84, 84, 4))\n",
    "    # The first hidden layer convolves 32 filters of 8 x 8 with stride 4,\n",
    "    # with the input image and applies a rectifier nonlinearity.\n",
    "    hidden1 = Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(8, 8),\n",
    "        strides=4,\n",
    "        activation=relu\n",
    "    )(inputs)\n",
    "    # The second hidden layer convolves 64 filters of 4 x 4 with stride 2,\n",
    "    # again followed by a rectifier nonlinearity.\n",
    "    hidden2 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(4, 4),\n",
    "        strides=2,\n",
    "        activation=relu\n",
    "    )(hidden1)\n",
    "    # This is followed by a third convolutional layer that convolves 64 filters of 3 x 3,\n",
    "    # with stride 1 followed by a rectifier. \n",
    "    hidden3 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=1,\n",
    "        activation=relu\n",
    "    )(hidden2)\n",
    "    flatten = Flatten()(hidden3)\n",
    "    # The final hidden layer is fully-connected and consists of 512 rectifier units.\n",
    "    hidden4 = Dense(units=512, activation=relu)(flatten)\n",
    "    # The output layer is a fully-connected linear layer with a single output for each valid action.\n",
    "    outputs = Dense(units=num_actions)(hidden4)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs, trainable=trainable)\n",
    "\n",
    "\n",
    "model = create_model(env.action_space.n)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.stack([preprocessed]*4, axis=2)\n",
    "sample = np.expand_dims(state, axis=0)\n",
    "print(sample.shape)\n",
    "\n",
    "q_value = model(sample)\n",
    "print(q_value)"
   ]
  },
  {
   "source": [
    "## Optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "source": [
    "## Behavior policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "epsilon_random_frames = 50000\n",
    "\n",
    "def epsilon_greedy_policy(model, state, epsilon):\n",
    "    _, num_actions = model.output_shape\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    if np.random.sample() < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    with tf.device('/CPU:0'):\n",
    "        return np.argmax(model(state, training=False), axis=1)[0]\n",
    "\n",
    "print(epsilon_greedy_policy(model, state, 1))\n",
    "print(epsilon_greedy_policy(model, state, 0))"
   ]
  },
  {
   "source": [
    "## Hyperparameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions                     = env.action_space.n\n",
    "agent_history_length            = 4\n",
    "action_repeat                   = 4\n",
    "minibatch_size                  = 32\n",
    "replay_memory_size              = 1000000\n",
    "replay_start_size               = 50000\n",
    "\n",
    "update_frequency                = 4\n",
    "target_network_update_frequency = 10000\n",
    "discount_factor                 = 0.99\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "obs_history         = deque(maxlen=agent_history_length)\n",
    "state_history       = deque(maxlen=replay_memory_size)\n",
    "action_history      = deque(maxlen=replay_memory_size)\n",
    "rewards_history     = deque(maxlen=replay_memory_size)\n",
    "state_next_history  = deque(maxlen=replay_memory_size)\n",
    "done_history        = deque(maxlen=replay_memory_size)"
   ]
  },
  {
   "source": [
    "## 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    # Define our metrics\n",
    "    rl_rewards  = tf.keras.metrics.Sum('Avg. Rewards', dtype=tf.float32)\n",
    "    rl_loss     = tf.keras.metrics.Mean('Avg. Loss', dtype=tf.float32)\n",
    "    rl_q_values = tf.keras.metrics.Mean('Avg. Q-value', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "@tf.function\n",
    "def train(state_sample, action_sample, rewards_sample, state_next_sample, done_sample):\n",
    "    state_sample = tf.convert_to_tensor(state_sample)\n",
    "    action_sample = tf.convert_to_tensor(action_sample)\n",
    "    rewards_sample = tf.convert_to_tensor(rewards_sample)\n",
    "    state_next_sample = tf.convert_to_tensor(state_next_sample)\n",
    "    done_sample = tf.convert_to_tensor(done_sample)\n",
    "\n",
    "    future_rewards = model_target(state_next_sample, training=False) * (1 - done_sample)\n",
    "    q_values_target = rewards_sample + discount_factor * tf.reduce_max(future_rewards, axis=1)\n",
    "\n",
    "    masks = tf.one_hot(action_sample, num_actions)\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(state_sample)\n",
    "        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "        loss = MSE(q_values_target, q_action)\n",
    "\n",
    "    # Backpropagation\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss, tf.reduce_max(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/DQN/' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = create_model(num_actions)\n",
    "model_target = create_model(num_actions, trainable=False)\n",
    "model_target.set_weights(model.get_weights())\n",
    "\n",
    "max_frame_count = 10000000\n",
    "\n",
    "frame_count = 0\n",
    "episode_count = 0\n",
    "\n",
    "while frame_count <= max_frame_count:\n",
    "    lives = 5\n",
    "    info = {'ale.lives': 5}\n",
    "    episode_count += 1\n",
    "\n",
    "    turn_done = True\n",
    "    episode_done = False\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = preprocessing(obs)\n",
    "\n",
    "    f_count = 0\n",
    "\n",
    "    while episode_done is False:\n",
    "        frame_count += 1\n",
    "\n",
    "        if turn_done:\n",
    "            for _ in range(agent_history_length):\n",
    "                obs_history.append(obs)\n",
    "            state = np.stack(obs_history, axis=2)\n",
    "            turn_done = False\n",
    "\n",
    "        if frame_count <= replay_start_size:\n",
    "            action = epsilon_greedy_policy(model, state, 1)\n",
    "        else:\n",
    "            action = epsilon_greedy_policy(model, state, epsilon)\n",
    "            epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        reward_signal = 0\n",
    "        for i in range(agent_history_length):\n",
    "            for _ in range(action_repeat):\n",
    "                f_count += 1\n",
    "                if lives > info['ale.lives']:\n",
    "                    turn_done = True\n",
    "                    break\n",
    "                # env.render()\n",
    "                obs, reward, episode_done, info = env.step(action)\n",
    "                obs = preprocessing(obs)\n",
    "                reward_signal += reward\n",
    "\n",
    "            obs_history.append(obs)\n",
    "\n",
    "        reward_signal = np.sign(reward_signal)\n",
    "\n",
    "        if turn_done or episode_done:\n",
    "            reward_signal = -1\n",
    "\n",
    "        state_next = np.stack(obs_history, axis=2)\n",
    "        done = 1.0 if turn_done or episode_done else 0.0\n",
    "\n",
    "        reward_signal = np.float32(reward_signal)\n",
    "        done = np.float32(done)\n",
    "\n",
    "        state_history.append(state)\n",
    "        action_history.append(action)\n",
    "        rewards_history.append(reward_signal)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append([done])\n",
    "\n",
    "\n",
    "        lives = info['ale.lives']\n",
    "        state = state_next\n",
    "\n",
    "\n",
    "        with tf.device('/CPU:0'):\n",
    "            rl_rewards(reward_signal)\n",
    "\n",
    "        if len(done_history) > replay_start_size:\n",
    "            for _ in range(update_frequency):\n",
    "                indices = np.random.choice(range(len(done_history)), size=minibatch_size)\n",
    "\n",
    "                state_sample        = np.stack([state_history[i] for i in indices])\n",
    "                action_sample       = np.stack([action_history[i] for i in indices])\n",
    "                rewards_sample      = np.stack([rewards_history[i] for i in indices])\n",
    "                state_next_sample   = np.stack([state_next_history[i] for i in indices])\n",
    "                done_sample         = np.stack([done_history[i] for i in indices])\n",
    "\n",
    "                loss, q_max = train(state_sample, action_sample, rewards_sample, state_next_sample, done_sample)\n",
    "\n",
    "                with tf.device('/CPU:0'):\n",
    "                    rl_loss(loss)\n",
    "                    rl_q_values(q_max)\n",
    "\n",
    "\n",
    "        if frame_count % target_network_update_frequency == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('DQN/Avg. Loss', rl_loss.result(), step=frame_count)\n",
    "            tf.summary.scalar('DQN/Avg. Q-value', rl_q_values.result(), step=frame_count)\n",
    "            tf.summary.scalar('DQN/Epsilon', data=epsilon, step=episode_count)\n",
    "            # tf.summary.scalar(\n",
    "            #     'DQN/GPU usages',\n",
    "            #     data=tf.config.experimental.get_memory_usage(\"GPU:0\"),\n",
    "            #     step=frame_count\n",
    "            # )\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('DQN/Avg. Rewards', rl_rewards.result(), step=episode_count)\n",
    "\n",
    "    print(\"Episode: {} | Loss: {} | Q-value: {} | Rewards: {} | epsilon: {}\".format(\n",
    "        episode_count, rl_loss.result(), rl_q_values.result(), rl_rewards.result(), epsilon\n",
    "    ))\n",
    "    \n",
    "    rl_loss.reset_states()\n",
    "    rl_q_values.reset_states()\n",
    "    rl_rewards.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10, 20):\n",
    "    print(action_history[idx], rewards_history[idx], done_history[idx])\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=4, figsize=(16, 16))\n",
    "    for i in range(4):\n",
    "        axs[i].set_axis_off()\n",
    "        axs[i].imshow(state_history[idx][:, :, i])\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=4, figsize=(16, 16))\n",
    "    for i in range(4):\n",
    "        axs[i].set_axis_off()\n",
    "        axs[i].imshow(state_next_history[idx][:, :, i])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Breakout-v4\")\n",
    "\n",
    "while True:\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = preprocessing(obs)\n",
    "\n",
    "    for _ in range(skip_frame):\n",
    "        obs_history.append(obs)\n",
    "\n",
    "    state = np.stack(obs_history, axis=2)\n",
    "\n",
    "    is_done = False\n",
    "\n",
    "    while is_done is False:\n",
    "        inputs = np.expand_dims(state, axis=0)\n",
    "        action = epsilon_greedy_policy(model, inputs, 0)\n",
    "\n",
    "        obs_next, reward, is_done, _ = env.step(action)\n",
    "        obs_next = preprocessing(obs_next)\n",
    "        obs_history.append(obs_next)\n",
    "\n",
    "        state_next = np.stack(obs_history, axis=2)\n",
    "        state = state_next"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Breakout-v4\")\n",
    "\n",
    "while True:\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = preprocessing(obs)\n",
    "\n",
    "    for _ in range(4):\n",
    "        obs_history.append(obs)\n",
    "\n",
    "    state = np.stack(obs_history, axis=2)\n",
    "\n",
    "    is_done = False\n",
    "\n",
    "    while is_done is False:\n",
    "        env.render()\n",
    "        action = epsilon_greedy_policy(model, state, 1)\n",
    "\n",
    "        obs_next, reward, is_done, _ = env.step(action)\n",
    "        # print(reward)\n",
    "        obs_next = preprocessing(obs_next)\n",
    "        obs_history.append(obs_next)\n",
    "        state_next = np.stack(obs_history, axis=2)\n",
    "        state = state_next\n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}