{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd072f82c1a1813237a015b4f4b2159de22a2fabef90219bd259332cc8881f19581",
   "display_name": "Python 3.9.2 64-bit ('py3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def preprocessing(image):\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    # cropping an 84 × 84 region of the image that roughly captures the playing area\n",
    "    (left, upper, right, lower) = (0, 17, 160, 17+177)\n",
    "    image = image.crop(box=(left, upper, right, lower))\n",
    "\n",
    "    # converting their RGB representation to gray-scale\n",
    "    image = image.convert(\"L\")\n",
    "    \n",
    "    # down-sampling it to a 110×84 image\n",
    "    (width, height) = (84, 84)\n",
    "    image = image.resize(size=(width, height))\n",
    "\n",
    "    # normalization\n",
    "    image = np.asarray(image, dtype=np.float32)\n",
    "    image /= 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.activations import relu\n",
    "\n",
    "\n",
    "def create_model(num_actions, trainable=True):\n",
    "    # The input to the neural network consists of an 84 x 84 x 4 image produced by the preprocessing map φ.\n",
    "    inputs = Input(shape=(10, 84, 84, 1))\n",
    "    # The first hidden layer convolves 32 filters of 8 x 8 with stride 4,\n",
    "    # with the input image and applies a rectifier nonlinearity.\n",
    "    hidden1 = Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(8, 8),\n",
    "        strides=4,\n",
    "        activation=relu\n",
    "    )(inputs)\n",
    "    # The second hidden layer convolves 64 filters of 4 x 4 with stride 2,\n",
    "    # again followed by a rectifier nonlinearity.\n",
    "    hidden2 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(4, 4),\n",
    "        strides=2,\n",
    "        activation=relu\n",
    "    )(hidden1)\n",
    "    # This is followed by a third convolutional layer that convolves 64 filters of 3 x 3,\n",
    "    # with stride 1 followed by a rectifier. \n",
    "    hidden3 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=1,\n",
    "        activation=relu\n",
    "    )(hidden2)\n",
    "\n",
    "    elems_size = tf.math.reduce_prod(hidden3.shape[1:])\n",
    "    flatten = tf.reshape(hidden3, [-1, 1, elems_size])\n",
    "    # replacing only its first fully connected layer with a recurrent LSTM layer of the same size.\n",
    "    hidden4 = LSTM(units=512)(flatten)\n",
    "    # The output layer is a fully-connected linear layer with a single output for each valid action.\n",
    "    outputs = Dense(units=num_actions)(hidden4)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs, trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.activations import relu\n",
    "\n",
    "\n",
    "def create_model(num_actions, trainable=True):\n",
    "    # The input to the neural network consists of an 84 x 84 x 4 image produced by the preprocessing map φ.\n",
    "    inputs = Input(shape=(10, 84, 84, 1))\n",
    "    # The first hidden layer convolves 32 filters of 8 x 8 with stride 4,\n",
    "    # with the input image and applies a rectifier nonlinearity.\n",
    "    hidden1 = TimeDistributed(Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(8, 8),\n",
    "        strides=4,\n",
    "        activation=relu\n",
    "    ))(inputs)\n",
    "    # The second hidden layer convolves 64 filters of 4 x 4 with stride 2,\n",
    "    # again followed by a rectifier nonlinearity.\n",
    "    hidden2 = TimeDistributed(Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(4, 4),\n",
    "        strides=2,\n",
    "        activation=relu\n",
    "    ))(hidden1)\n",
    "    # This is followed by a third convolutional layer that convolves 64 filters of 3 x 3,\n",
    "    # with stride 1 followed by a rectifier. \n",
    "    hidden3 = TimeDistributed(Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=1,\n",
    "        activation=relu\n",
    "    ))(hidden2)\n",
    "\n",
    "    flatten = TimeDistributed(Flatten())(hidden3)\n",
    "    # replacing only its first fully connected layer with a recurrent LSTM layer of the same size.\n",
    "    hidden4 = LSTM(units=512)(flatten)\n",
    "    # The output layer is a fully-connected linear layer with a single output for each valid action.\n",
    "    outputs = Dense(units=num_actions)(hidden4)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs, trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Breakout-v4\")\n",
    "obs = env.reset()\n",
    "\n",
    "model = create_model(env.action_space.n)\n",
    "model.summary()\n",
    "\n",
    "preprocessed = preprocessing(obs)\n",
    "\n",
    "state = np.expand_dims([preprocessed]*10, axis=-1)\n",
    "sample = np.expand_dims(state, axis=0)\n",
    "\n",
    "model(sample)"
   ]
  },
  {
   "source": [
    "## Optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adadelta\n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.95\n",
    "\n",
    "optimizer = Adadelta(learning_rate=learning_rate, rho=momentum)"
   ]
  },
  {
   "source": [
    "## Behavior policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "\n",
    "def epsilon_greedy_policy(model, state, epsilon):\n",
    "    _, num_actions = model.output_shape\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    if np.random.sample() < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    with tf.device('/CPU:0'):\n",
    "        return np.argmax(model(state, training=False), axis=1)[0]\n",
    "\n",
    "print(epsilon_greedy_policy(model, state, 1))\n",
    "print(epsilon_greedy_policy(model, state, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions                     = env.action_space.n\n",
    "agent_history_length            = 10\n",
    "action_repeat                   = 0\n",
    "minibatch_size                  = 32\n",
    "replay_memory_size              = 1000000\n",
    "replay_start_size               = 50000\n",
    "replay_start_size               = 1000\n",
    "\n",
    "update_frequency                = 4\n",
    "target_network_update_frequency = 10000\n",
    "discount_factor                 = 0.99\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "obs_history         = deque(maxlen=agent_history_length)\n",
    "state_history       = deque(maxlen=replay_memory_size)\n",
    "action_history      = deque(maxlen=replay_memory_size)\n",
    "rewards_history     = deque(maxlen=replay_memory_size)\n",
    "state_next_history  = deque(maxlen=replay_memory_size)\n",
    "done_history        = deque(maxlen=replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    # Define our metrics\n",
    "    rl_rewards  = tf.keras.metrics.Sum('Avg. Rewards', dtype=tf.float32)\n",
    "    rl_loss     = tf.keras.metrics.Mean('Avg. Loss', dtype=tf.float32)\n",
    "    rl_q_values = tf.keras.metrics.Mean('Avg. Q-value', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "@tf.function\n",
    "def train(state_sample, action_sample, rewards_sample, state_next_sample, done_sample):\n",
    "    future_rewards = model_target(state_next_sample, training=False) * (1 - done_sample)\n",
    "    q_values_target = rewards_sample + discount_factor * tf.reduce_max(future_rewards, axis=1)\n",
    "\n",
    "    masks = tf.one_hot(action_sample, num_actions)\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(state_sample)\n",
    "        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "        loss = MSE(q_values_target, q_action)\n",
    "\n",
    "    # Backpropagation\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss, tf.reduce_max(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/DRQN/' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = create_model(num_actions)\n",
    "model_target = create_model(num_actions, trainable=False)\n",
    "model_target.set_weights(model.get_weights())\n",
    "\n",
    "max_frame_count = 10000000\n",
    "\n",
    "frame_count = 0\n",
    "episode_count = 0\n",
    "\n",
    "while frame_count <= max_frame_count:\n",
    "    lives = 5\n",
    "    info = {'ale.lives': 5}\n",
    "    episode_count += 1\n",
    "\n",
    "    turn_done = True\n",
    "    episode_done = False\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = preprocessing(obs)\n",
    "\n",
    "\n",
    "    while episode_done is False:\n",
    "        frame_count += 1\n",
    "\n",
    "        if turn_done:\n",
    "            state_step, action_step, state_next_step, rewards_step, done_step = [], [], [], [], []\n",
    "            for _ in range(agent_history_length):\n",
    "                obs_history.append(obs)\n",
    "            state = np.expand_dims(obs_history, axis=-1)\n",
    "            turn_done = False\n",
    "\n",
    "        if frame_count <= replay_start_size:\n",
    "            action = epsilon_greedy_policy(model, state, 1)\n",
    "        else:\n",
    "            action = epsilon_greedy_policy(model, state, epsilon)\n",
    "            epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        reward_signal = 0\n",
    "        \n",
    "        # env.render()\n",
    "        obs, reward, episode_done, info = env.step(action)\n",
    "        obs = preprocessing(obs)\n",
    "        reward_signal += reward\n",
    "\n",
    "        if lives > info['ale.lives']:\n",
    "            turn_done = True\n",
    "\n",
    "        if turn_done or episode_done:\n",
    "            reward_signal -= -1\n",
    "\n",
    "        reward_signal = np.sign(reward_signal)\n",
    "\n",
    "        state_next = np.expand_dims(obs_history, axis=-1)\n",
    "        done = 1.0 if turn_done or episode_done else 0.0\n",
    "\n",
    "        reward_signal = np.float32(reward_signal)\n",
    "        done = np.float32(done)\n",
    "\n",
    "        state_step.append(state)\n",
    "        action_step.append(action)\n",
    "        rewards_step.append(reward_signal)\n",
    "        state_next_step.append(state_next)\n",
    "        done_step.append([done])\n",
    "\n",
    "\n",
    "        lives = info['ale.lives']\n",
    "        state = state_next\n",
    "\n",
    "\n",
    "        with tf.device('/CPU:0'):\n",
    "            rl_rewards(reward_signal)\n",
    "\n",
    "\n",
    "\n",
    "        if frame_count > replay_start_size:\n",
    "            for _ in range(update_frequency):\n",
    "                indices = np.random.choice(range(len(done_history)), size=minibatch_size)\n",
    "\n",
    "                state_sample, action_sample, rewards_sample, state_next_sample, done_sample = [], [], [], [], []\n",
    "                for idx in indices:\n",
    "                    step_idx = np.random.randint(len(done_history[idx]))\n",
    "                    state_sample.append(state_history[idx][step_idx])\n",
    "                    action_sample.append(action_history[idx][step_idx])\n",
    "                    rewards_sample.append(rewards_history[idx][step_idx])\n",
    "                    state_next_sample.append(state_next_history[idx][step_idx])\n",
    "                    done_sample.append(done_history[idx][step_idx])\n",
    "\n",
    "                state_sample = np.array(state_sample)\n",
    "                action_sample = np.array(action_sample)\n",
    "                rewards_sample = np.array(rewards_sample)\n",
    "                state_next_sample = np.array(state_next_sample)\n",
    "                done_sample = np.array(done_sample)\n",
    "\n",
    "                loss, q_max = train(state_sample, action_sample, rewards_sample, state_next_sample, done_sample)\n",
    "\n",
    "                with tf.device('/CPU:0'):\n",
    "                    rl_loss(loss)\n",
    "                    rl_q_values(q_max)\n",
    "\n",
    "\n",
    "        if frame_count % target_network_update_frequency == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('DRQN/Avg. Loss', rl_loss.result(), step=frame_count)\n",
    "            tf.summary.scalar('DRQN/Avg. Q-value', rl_q_values.result(), step=frame_count)\n",
    "            tf.summary.scalar('DRQN/Epsilon', data=epsilon, step=frame_count)\n",
    "            # tf.summary.scalar(\n",
    "            #     'DRQN/GPU usages',\n",
    "            #     data=tf.config.experimental.get_memory_usage(\"GPU:0\"),\n",
    "            #     step=frame_count\n",
    "            # )\n",
    "\n",
    "\n",
    "    state_history.append(state_step)\n",
    "    action_history.append(action_step)\n",
    "    rewards_history.append(rewards_step)\n",
    "    state_next_history.append(state_next_step)\n",
    "    done_history.append(done_step)\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('DRQN/Avg. Rewards', rl_rewards.result(), step=episode_count)\n",
    "\n",
    "    print(\"Episode: {} | Loss: {} | Q-value: {} | Rewards: {} | epsilon: {}\".format(\n",
    "        episode_count, rl_loss.result(), rl_q_values.result(), rl_rewards.result(), epsilon\n",
    "    ))\n",
    "    \n",
    "    rl_loss.reset_states()\n",
    "    rl_q_values.reset_states()\n",
    "    rl_rewards.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}