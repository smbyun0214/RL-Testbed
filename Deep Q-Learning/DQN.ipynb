{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd072f82c1a1813237a015b4f4b2159de22a2fabef90219bd259332cc8881f19581",
   "display_name": "Python 3.9.2 64-bit ('py3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "OpenAI Gym에서 \"Breakout-v4\" 게임을 플레이하기 위해서는 `atari-py`를 설치해야 한다.\n",
    "\n",
    "```Python\n",
    "pip install gym atari-py\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 이미지 전처리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Breakout-v4\")\n",
    "obs = env.reset()\n",
    "\n",
    "plt.matshow(obs)\n",
    "plt.show()\n",
    "\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def preprocessing(image):\n",
    "    # converting their RGB representation to gray-scale\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    # down-sampling it to a 110×84 image\n",
    "    image = tf.image.resize(image, size=[110, 84])\n",
    "    # cropping an 84 × 84 region of the image that roughly captures the playing area\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        image, \n",
    "        offset_height=17, offset_width=0,\n",
    "        target_height=84, target_width=84\n",
    "    )\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    return tf.squeeze(image)\n",
    "\n",
    "preprocessed = preprocessing(obs)\n",
    "print(preprocessed.shape)\n",
    "\n",
    "plt.matshow(preprocessed)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.activations import relu\n",
    "\n",
    "\n",
    "def create_model(output_size, trainable=True):\n",
    "    # The input to the neural network consists is an 84 × 84 × 4 image produced by φ.\n",
    "    inputs = Input(shape=(84, 84, 4))\n",
    "    # inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
    "    # The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image,\n",
    "    # and applies a rectifier nonlinearity [10, 18]. \n",
    "    hidden1 = Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=(8, 8),\n",
    "        strides=4,\n",
    "        activation=relu\n",
    "    )(inputs)\n",
    "    # The second hidden layer convolves 32 4 × 4 filters with stride 2,\n",
    "    # again followed by a rectifier nonlinearity.\n",
    "    hidden2 = Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(4, 4),\n",
    "        strides=2,\n",
    "        activation=relu\n",
    "    )(hidden1)\n",
    "    # The final hidden layer is fully-connected and consists of 256 rectifier units.\n",
    "    flatten = Flatten()(hidden2)\n",
    "    hidden3 = Dense(units=256, activation=relu)(flatten)\n",
    "    # The output layer is a fully-connected linear layer with a single output for each valid action.\n",
    "    outputs = Dense(units=output_size)(hidden3)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs, trainable=trainable)\n",
    "\n",
    "\n",
    "model = create_model(env.action_space.n)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tf.stack([preprocessed]*4, axis=2)\n",
    "sample = tf.expand_dims(sample, axis=0)\n",
    "print(sample.shape)\n",
    "\n",
    "q_value = model(sample)\n",
    "print(q_value)"
   ]
  },
  {
   "source": [
    "## Optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "optimizer = RMSprop()"
   ]
  },
  {
   "source": [
    "## Behavior policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "epsilon_random_frames = 50000\n",
    "\n",
    "def epsilon_greedy_policy(model, state, epsilon):\n",
    "    _, num_actions = model.output_shape\n",
    "    if tf.random.uniform(shape=[]) < epsilon:\n",
    "        return tf.random.uniform(shape=[], maxval=num_actions, dtype=tf.int64)\n",
    "    return tf.math.argmax(model(state), axis=1, output_type=tf.int64)[0]\n",
    "\n",
    "print(epsilon_greedy_policy(model, sample, 1))\n",
    "print(epsilon_greedy_policy(model, sample, 0))"
   ]
  },
  {
   "source": [
    "## Hyperparameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "skip_frame = 4\n",
    "replay_memory_length = 1000000\n",
    "update_target_network = 10000\n",
    "max_frame_count = 10000000\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "obs_history         = deque(maxlen=skip_frame)\n",
    "state_history       = deque(maxlen=replay_memory_length)\n",
    "action_history      = deque(maxlen=replay_memory_length)\n",
    "rewards_history     = deque(maxlen=replay_memory_length)\n",
    "state_next_history  = deque(maxlen=replay_memory_length)\n",
    "done_history        = deque(maxlen=replay_memory_length)"
   ]
  },
  {
   "source": [
    "## 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "# Define our metrics\n",
    "rl_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "rl_rewards = tf.keras.metrics.Sum('avg. rewards', dtype=tf.float32)\n",
    "rl_q_values = tf.keras.metrics.Mean('avg. Q', dtype=tf.float32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(state_sample, action_sample, rewards_sample, state_next_sample, done_sample):\n",
    "    # Build the updated Q-values for the sampled future states\n",
    "    # Use the target model for stability\n",
    "    future_rewards = model_target(state_next_sample) * (1 - done_sample)\n",
    "    # Q value = reward + discount factor * expected future reward\n",
    "    q_values_target = rewards_sample + gamma * tf.reduce_max(future_rewards)\n",
    "\n",
    "    # Create a mask so we only calculate loss on the updated Q-values\n",
    "    masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Train the model on the states and updated Q-values\n",
    "        q_values = model(state_sample)\n",
    "\n",
    "        # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "        # Calculate loss between new Q-value and old Q-value\n",
    "        loss = MSE(q_values_target, q_action)\n",
    "\n",
    "    rl_loss(loss)\n",
    "    \n",
    "    q_max = tf.reduce_max(q_values)\n",
    "    rl_q_values(q_max)\n",
    "\n",
    "    # Backpropagation\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/DQN/' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "model = create_model(num_actions)\n",
    "model_target = create_model(num_actions, trainable=False)\n",
    "model_target.set_weights(model.get_weights())\n",
    "\n",
    "frame_count = 0\n",
    "episode_count = 0\n",
    "\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "\n",
    "while frame_count <= max_frame_count:\n",
    "    episode_count += 1\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = preprocessing(obs)\n",
    "\n",
    "    for _ in range(skip_frame):\n",
    "        obs_history.append(obs)\n",
    "\n",
    "    state = tf.stack(obs_history, axis=2)\n",
    "\n",
    "    is_done = False\n",
    "\n",
    "    while is_done is False:\n",
    "        frame_count += 1\n",
    "\n",
    "        inputs = tf.expand_dims(state, axis=0)\n",
    "\n",
    "        if frame_count <= epsilon_random_frames:\n",
    "            action = epsilon_greedy_policy(model, inputs, 1)\n",
    "        else:\n",
    "            action = epsilon_greedy_policy(model, inputs, epsilon)\n",
    "            # Decay probability of taking random action\n",
    "            epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        reward = 0\n",
    "        for _ in range(skip_frame):\n",
    "            if is_done is False:\n",
    "                # env.render()\n",
    "                obs_next, _reward, is_done, _ = env.step(action)\n",
    "                obs_next = preprocessing(obs_next)\n",
    "            reward += _reward\n",
    "            obs_history.append(obs_next)\n",
    "\n",
    "        state_next = tf.stack(obs_history, axis=2)\n",
    "        done = 1.0 if is_done else 0.0\n",
    "\n",
    "        state_history.append(state)\n",
    "        action_history.append(action)\n",
    "        rewards_history.append(reward)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append([done])\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "        if len(done_history) > epsilon_random_frames:\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            state_sample        = tf.stack([state_history[i] for i in indices])\n",
    "            action_sample       = tf.stack([action_history[i] for i in indices])\n",
    "            rewards_sample      = tf.stack([rewards_history[i] for i in indices])\n",
    "            state_next_sample   = tf.stack([state_next_history[i] for i in indices])\n",
    "            done_sample         = tf.stack([done_history[i] for i in indices])\n",
    "\n",
    "            train(state_sample, action_sample, rewards_sample, state_next_sample, done_sample)\n",
    "\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('DQN/Loss', rl_loss.result(), step=frame_count)\n",
    "                tf.summary.scalar('DQN/Average Q', rl_q_values.result(), step=frame_count)\n",
    "\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "\n",
    "        rl_rewards(reward)\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('DQN/Average Rewards', rl_rewards.result(), step=episode_count)\n",
    "        tf.summary.scalar('DQN/Epsilon', data=epsilon, step=episode_count)\n",
    "\n",
    "    rl_loss.reset_states()\n",
    "    rl_q_values.reset_states()\n",
    "    rl_rewards.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Breakout-v4\")\n",
    "\n",
    "while True:\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = preprocessing(obs)\n",
    "\n",
    "    for _ in range(skip_frame):\n",
    "        obs_history.append(obs)\n",
    "\n",
    "    state = tf.stack(obs_history, axis=2)\n",
    "\n",
    "    is_done = False\n",
    "\n",
    "    while is_done is False:\n",
    "        inputs = tf.expand_dims(state, axis=0)\n",
    "        action = epsilon_greedy_policy(model, inputs, 0)\n",
    "\n",
    "        obs_next, _, is_done, _ = env.step(action)\n",
    "        obs_next = preprocessing(obs_next)\n",
    "        obs_history.append(obs_next)\n",
    "\n",
    "        state_next = tf.stack(obs_history, axis=2)\n",
    "        state = state_next"
   ]
  }
 ]
}